# Memory-Efficient NetCDF Environmental Flow Calculator
import numpy as np
import xarray as xr
import calendar
from pathlib import Path
import gc
import shutil
import dask.array as da

class NetCDFEnvironmentalFlowCalculator:
    """
    Memory-efficient calculator for environmental flow metrics from NetCDF files.
    The algorithms in this class are designed to precisely match the logic of the
    single-file EnvironmentalFlowCalculator for consistent results.
    """
    
    DEFAULT_PERCENTILES = [0.01, 0.1, 1, 5, 10, 20, 30, 40, 50, 
                           60, 70, 80, 90, 95, 99, 99.9, 99.99]
    FLOW_CLASSES = ['A', 'B', 'C', 'D', 'E', 'F']

    def __init__(self, data_dir: str, variable_name: str = 'discharge', chunk_size: int = 100):
        self.data_dir = Path(data_dir)
        self.variable_name = variable_name
        self.chunk_size = chunk_size
        self.discharge_data = None
        self.dimensions = {}
        
        self._load_and_validate_data()
        print(f"Data loaded starting from time {self.discharge_data.time.values[0]} with dimensions: {self.discharge_data.dims}")
        print(f"Memory per chunk: ~{self._estimate_chunk_memory():.2f} MB")

    def _load_and_validate_data(self) -> None:
        nc_files = sorted(self.data_dir.glob('*/*.nc'))
        if not nc_files:
            raise ValueError(f"No NetCDF files found in subdirectories of {self.data_dir}")

        self.discharge_data = xr.open_mfdataset(
            nc_files, 
            combine='by_coords',
        ).sortby('time')
        
        self._set_dimension_names()
        
        chunk_config = {
            self.dimensions['lat']: self.chunk_size,
            self.dimensions['lon']: self.chunk_size,
            self.dimensions['time']: -1
        }
        self.discharge_data = self.discharge_data.chunk(chunk_config)
        
        if self.variable_name not in self.discharge_data:
            raise ValueError(f"Variable '{self.variable_name}' not found")

    def _set_dimension_names(self) -> None:
        self.dimensions = {
            'lat': next((dim for dim in ['lat', 'latitude'] if dim in self.discharge_data.dims), None),
            'lon': next((dim for dim in ['lon', 'longitude'] if dim in self.discharge_data.dims), None),
            'time': 'time'
        }
        if not self.dimensions['lat'] or not self.dimensions['lon']:
            raise ValueError("Could not automatically determine latitude/longitude dimension names.")

    def _estimate_chunk_memory(self) -> float:
        time_size = len(self.discharge_data[self.dimensions['time']])
        chunk_elements = self.chunk_size**2 * time_size
        return (chunk_elements * 8) / (1024**2)

    def save_results(self, output_dir: str, metrics: dict) -> None:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"Processing grid: {self.discharge_data.sizes[self.dimensions['lat']]}x{self.discharge_data.sizes[self.dimensions['lon']]} with {self.chunk_size} chunk size")
        
        if metrics.get('mar'):
            self._process_mar(output_dir)
        if metrics.get('fdc'):
            self._process_fdc(output_dir)
        if metrics.get('flow_classes'):
            self._process_flow_classes(output_dir)

    def _process_mar(self, output_dir: Path) -> None:
        print("Calculating Mean Annual Runoff...")
        
        lat_dim, lon_dim = self.dimensions['lat'], self.dimensions['lon']
        input_da = self.discharge_data[self.variable_name]

        output_shape = (input_da.sizes[lat_dim], input_da.sizes[lon_dim])
        output_chunks = (input_da.chunksizes[lat_dim], input_da.chunksizes[lon_dim])
        
        dask_template_array = da.empty(shape=output_shape, chunks=output_chunks, dtype=np.float32)
        
        template = xr.DataArray(
            dask_template_array,
            dims=[lat_dim, lon_dim],
            coords={
                lat_dim: input_da.coords[lat_dim],
                lon_dim: input_da.coords[lon_dim]
            }
        )
        
        mar_result = xr.map_blocks(
            self._calculate_mar_chunk,
            input_da,
            template=template,
        ).compute()

        self._save_mar_results(mar_result, output_dir)
        
    def _calculate_mar_chunk(self, chunk: xr.DataArray) -> xr.DataArray:
        lat_dim, lon_dim = self.dimensions['lat'], self.dimensions['lon']
        
        if chunk.dims.index('time') != 0:
            chunk = chunk.transpose('time', ...)
            
        discharge = chunk.values
        time_coords = chunk[self.dimensions['time']]
        
        monthly_runoff = discharge * 30 * 86400 / 1e6

        years = time_coords.dt.year.values
        unique_years = np.unique(years)
        annual_runoff = []
        
        for year in unique_years:
            year_mask = (years == year)
            n_months = np.sum(year_mask)
            if n_months == 0: continue
                
            year_sum = np.nansum(monthly_runoff[year_mask, ...], axis=0)
            if n_months < 12:
                year_sum *= (12 / n_months)
            annual_runoff.append(year_sum)
        
        if not annual_runoff:
            result_np = np.full(discharge.shape[1:], np.nan, dtype=np.float32)
        else:
            result_np = np.nanmean(np.stack(annual_runoff, axis=0), axis=0).astype(np.float32)
        
        return xr.DataArray(
            result_np,
            dims=[lat_dim, lon_dim],
            coords={
                lat_dim: chunk.coords[lat_dim],
                lon_dim: chunk.coords[lon_dim]
            }
        )

    def _save_mar_results(self, data: xr.DataArray, output_dir: Path) -> None:
        data.attrs={'units': 'MCM', 'long_name': 'Mean Annual Runoff'}
        mar_ds = xr.Dataset({'mar': data})
        mar_path = output_dir / 'mean_annual_runoff.nc'
        mar_ds.to_netcdf(mar_path, encoding={'mar': {'zlib': True, 'complevel': 5}})
        print(f"MAR results saved to {mar_path}")

    def _process_fdc(self, output_dir: Path) -> None:
        print("Calculating Flow Duration Curve...")
        
        lat_dim, lon_dim = self.dimensions['lat'], self.dimensions['lon']
        input_da = self.discharge_data[self.variable_name]

        output_shape = (len(self.DEFAULT_PERCENTILES), input_da.sizes[lat_dim], input_da.sizes[lon_dim])
        output_chunks = (len(self.DEFAULT_PERCENTILES), input_da.chunksizes[lat_dim], input_da.chunksizes[lon_dim])
        
        dask_template_array = da.empty(shape=output_shape, chunks=output_chunks, dtype=np.float32)
        
        template = xr.DataArray(
            dask_template_array,
            dims=['percentile', lat_dim, lon_dim],
            coords={
                'percentile': self.DEFAULT_PERCENTILES,
                lat_dim: input_da.coords[lat_dim],
                lon_dim: input_da.coords[lon_dim]
            }
        )
        
        fdc_results = xr.map_blocks(
            self._calculate_fdc_chunk,
            input_da,
            template=template
        ).compute()

        self._save_fdc_results(fdc_results, output_dir)
        
    def _calculate_fdc_chunk(self, chunk: xr.DataArray) -> xr.DataArray:
        lat_dim, lon_dim = self.dimensions['lat'], self.dimensions['lon']

        if chunk.dims.index('time') != 0:
            chunk = chunk.transpose('time', ...)
            
        discharge = chunk.values
        time_coords = chunk[self.dimensions['time']]
        _, lat_size, lon_size = discharge.shape
        
        fdc_results = np.full((len(self.DEFAULT_PERCENTILES), lat_size, lon_size), np.nan, dtype=np.float32)
        days_in_month = time_coords.dt.days_in_month.values
        
        for i in range(lat_size):
            for j in range(lon_size):
                series = discharge[:, i, j]
                valid_mask = ~np.isnan(series) & (series > 0)
                
                if not np.any(valid_mask): continue

                valid_series = series[valid_mask]
                valid_days = days_in_month[valid_mask]
                
                monthly_volumes = valid_series * valid_days * 86400 / 1e6
                if monthly_volumes.size == 0: continue
                
                sorted_volumes = np.sort(monthly_volumes)[::-1]
                n = len(sorted_volumes)
                exceedance_prob = 100 * (np.arange(1, n + 1)) / (n + 1)
                log_volumes = np.log10(sorted_volumes)
                
                for p_idx, percentile in enumerate(self.DEFAULT_PERCENTILES):
                    log_val = self._interpolate_fdc_pixel(percentile, exceedance_prob, log_volumes)
                    fdc_results[p_idx, i, j] = 10**log_val
        
        return xr.DataArray(
            fdc_results,
            dims=['percentile', lat_dim, lon_dim],
            coords={
                'percentile': self.DEFAULT_PERCENTILES,
                lat_dim: chunk.coords[lat_dim],
                lon_dim: chunk.coords[lon_dim]
            }
        )
        
    def _interpolate_fdc_pixel(self, percentile, xp, yp):
        if len(yp) < 2:
            return yp[0] # Not enough points to interpolate/extrapolate, return the only value
        if percentile <= xp[0]:
            return self._left_extrapolate(percentile, xp, yp)
        if percentile >= xp[-1]:
            return self._right_extrapolate(percentile, xp, yp)
        return np.interp(percentile, xp, yp)

    def _left_extrapolate(self, x, xp, yp):
        # FIX: Added a check for the number of points
        if len(yp) < 2:
            return yp[0]
        slope = (yp[0] - yp[1]) / (xp[0] - xp[1])
        return yp[0] + slope * (x - xp[0])

    def _right_extrapolate(self, x, xp, yp):
        # FIX: Added a check for the number of points
        if len(yp) < 2:
            return yp[-1]
        slope = (yp[-1] - yp[-2]) / (xp[-1] - xp[-2])
        return yp[-1] + slope * (x - xp[-1])

    def _save_fdc_results(self, data: xr.DataArray, output_dir: Path):
        fdc_path = output_dir / 'flow_duration_curve.nc'
        data.attrs = {'units': 'MCM/month', 'long_name': 'Flow Duration Curve'}
        ds = xr.Dataset({'fdc': data})
        ds.to_netcdf(fdc_path, encoding={'fdc': {'zlib': True, 'complevel': 5}})
        print(f"FDC results saved to {fdc_path}")

    def _process_flow_classes(self, output_dir: Path) -> None:
        """
        OPTIMIZED: Calculate flow classes by applying a function to each FDC chunk.
        This avoids creating a complex Dask graph and is significantly faster.
        """
        print("Deriving flow classes from FDC (Optimized)...")
        fdc_path = output_dir / 'flow_duration_curve.nc'
        if not fdc_path.exists():
            print("Error: FDC file not found. Calculate FDC first.")
            return

        with xr.open_dataset(fdc_path, chunks={'lat': self.chunk_size, 'lon': self.chunk_size}) as fdc_ds:
            # Rechunk the input to have a single block along 'percentile'.
            # This ensures the entire percentile series is passed to the function for each spatial chunk.
            base_flows = fdc_ds['fdc'].chunk({'percentile': -1})
            
            lat_dim, lon_dim = self.dimensions['lat'], self.dimensions['lon']
            class_dim_name = 'flow_class' # Use a non-keyword name for the dimension

            # 1. Define the template for the output of map_blocks
            output_shape = (
                len(self.FLOW_CLASSES),
                len(self.DEFAULT_PERCENTILES),
                base_flows.sizes[lat_dim],
                base_flows.sizes[lon_dim]
            )
            output_chunks = (
                len(self.FLOW_CLASSES), # Class dimension is not chunked
                len(self.DEFAULT_PERCENTILES), # Percentile is not chunked
                base_flows.chunksizes[lat_dim],
                base_flows.chunksizes[lon_dim]
            )
            dask_template_array = da.empty(shape=output_shape, chunks=output_chunks, dtype=np.float32)
            
            template = xr.DataArray(
                dask_template_array,
                dims=[class_dim_name, 'percentile', lat_dim, lon_dim],
                coords={
                    class_dim_name: self.FLOW_CLASSES,
                    'percentile': self.DEFAULT_PERCENTILES,
                    lat_dim: base_flows.coords[lat_dim],
                    lon_dim: base_flows.coords[lon_dim]
                }
            )

            # 2. Apply the chunk-wise calculation using map_blocks
            all_classes_da = xr.map_blocks(
                self._calculate_flow_classes_chunk,
                base_flows,
                template=template
            ).compute()

            # 3. Convert the single DataArray with a 'class' dimension into a Dataset
            class_ds = xr.Dataset({
                cls: all_classes_da.sel({class_dim_name: cls}).drop_vars(class_dim_name) 
                for cls in self.FLOW_CLASSES
            })
            
            # 4. Save the final result
            class_path = output_dir / 'flow_classes.nc'
            encoding = {cls: {'zlib': True, 'complevel': 5} for cls in self.FLOW_CLASSES}
            class_ds.to_netcdf(class_path, encoding=encoding)
            print(f"Flow classes saved to {class_path}")

    def _calculate_flow_classes_chunk(self, fdc_chunk: xr.DataArray) -> xr.DataArray:
        """
        Operates on a single spatial chunk of FDC data (in-memory).
        This function contains the core loop, which is now fast.
        """
        _, lat_size, lon_size = fdc_chunk.shape
        lat_dim, lon_dim = self.dimensions['lat'], self.dimensions['lon']
        class_dim_name = 'flow_class' # Use the same non-keyword name

        # Initialize the output array for this chunk
        class_results = np.full(
            (len(self.FLOW_CLASSES), len(self.DEFAULT_PERCENTILES), lat_size, lon_size), 
            np.nan, 
            dtype=np.float32
        )

        # Loop over each pixel within the chunk
        for i in range(lat_size):
            for j in range(lon_size):
                base_flows_pixel = fdc_chunk[:, i, j].values
                
                if np.isnan(base_flows_pixel).all():
                    continue

                for cls_idx, _ in enumerate(self.FLOW_CLASSES):
                    truncated = base_flows_pixel[cls_idx + 1:]
                    num_to_extrapolate = cls_idx + 1
                    extrapolated = self._extrapolate_tail_pixel(truncated, num=num_to_extrapolate)
                    
                    class_flows_pixel = np.concatenate([truncated, extrapolated])
                    class_results[cls_idx, :, i, j] = class_flows_pixel
        
        # Wrap in a DataArray with the correct coordinates and dimensions
        return xr.DataArray(
            class_results,
            dims=[class_dim_name, 'percentile', lat_dim, lon_dim],
            coords={
                class_dim_name: self.FLOW_CLASSES,
                'percentile': self.DEFAULT_PERCENTILES,
                lat_dim: fdc_chunk.coords[lat_dim],
                lon_dim: fdc_chunk.coords[lon_dim]
            }
        )

    def _extrapolate_tail_pixel(self, values: np.ndarray, num: int) -> np.ndarray:
        """Extrapolates the tail for a single 1D NumPy array of flow values."""
        if len(values) < 2:
            last_val = values[-1] if len(values) > 0 else np.nan
            return np.full(num, last_val)

        # Use log space for linear extrapolation
        log_values = np.log10(values)
        
        # Calculate slope from the last two points
        slope = log_values[-1] - log_values[-2]
        
        # Extrapolate 'num' new points
        extrapolated_log = [log_values[-1] + slope * (i + 1) for i in range(num)]
        
        return 10**np.array(extrapolated_log)

if __name__ == "__main__":
    # data_dir = '../High_spatial_data_demo'
    data_dir = '../High_spatial_data1/output'
    output_dir = 'output_results1'
    
    if Path(output_dir).exists():
        shutil.rmtree(output_dir)
        
    calculator = NetCDFEnvironmentalFlowCalculator(
        data_dir=data_dir,
        variable_name='discharge',
        chunk_size=50
    )
    
    calculator.save_results(
        output_dir=output_dir,
        metrics={'mar': True, 'fdc': True, 'flow_classes': True}
    )
    
    print(f"Processing complete. Results saved to '{output_dir}/'")
